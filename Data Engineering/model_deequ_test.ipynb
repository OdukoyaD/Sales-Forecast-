{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36640d4",
   "metadata": {},
   "source": [
    "# DEEQU:\n",
    "Deequ is a library that has been built on top of Apache Spark. Its purpose is to define \"unit tests for data\" so that you       can measure the quality of large datasets.\n",
    "\n",
    "If using Deequ within a Python environment, there is a library called PyDeequ.\n",
    "\n",
    "This tool is specifically made for data quality testing and tracking.\n",
    "Seamless integration with Spark as it is built on top of Spark.\n",
    "Wide variety of built-in functions for data quality testing.\n",
    "Deequ provides a Profiler to automatically determine tests for your dataset using historical data.\n",
    "\n",
    "## The six dimensions of data quality are:\n",
    "\n",
    "### Accuracy:  \n",
    "The accuracy of data is the degree to which the data represent a real-world event or object.\n",
    "\n",
    "### Consistency:\n",
    "Consistency is the absence of difference when comparing two or more representations of something against a               reference. If data are recorded or captured in multiple places, consistency becomes very important. One cannot have             the same data point recorded in various ways.\n",
    "\n",
    "### Timeliness:\n",
    "Timeliness is the degree to which data represent reality from the required point in time. Timeliness expects that              the data within your dataset is sufficiently up to date. What are the delays between an event happening and the data            point being recorded?\n",
    "\n",
    "### Validity:\n",
    "The validity of a dataset is specific to a certain field. In other words, data is valid if it conforms to the syntax            (format, type, range) of its definition. Each field will have a property that makes it valid, such as an \"@\" symbol            for an email address.\n",
    "\n",
    "### Completeness:\n",
    "The completeness of data relates to how many values may be missing in your dataset.\n",
    "\n",
    "### Uniqueness:\n",
    "This dimension relates to having a real-world object or event represented only once in a particular dataset. The                same object cannot be duplicated. In other words, uniqueness specifies that nothing will be recorded more than once            based upon how that thing is identified. It is the inverse of an assessment of the level of duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b76ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\deequ\\lib\\site-packages\\pyspark\\__init__.py\n",
      "C:\\Users\\user\\anaconda3\\envs\\deequ\\lib\\site-packages\\pyspark\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "print(pyspark.__file__)\n",
    "path = os.path.abspath(pyspark.__file__)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f70950e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.0.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0bf8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pydeequ\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.profiles import *\n",
    "from pydeequ.suggestions import *\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DecimalType, DoubleType, IntegerType, DateType, NumericType, StructType, StringType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38aa4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory = '6g'\n",
    "#pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7558d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3b78bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-RS6OS5N:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d5e8dd7a30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c1b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spark.read.parquet('model_parquet.parquet')\n",
    "#validation_df = spark.read.parquet('validation_parquet.parquet')\n",
    "#evaluation_df = spark.read.parquet('evaluation_parquet.parquet')\n",
    "#cal_df = spark.read.parquet('calendar_parquet.parquet')\n",
    "#sellprice_df = spark.read.parquet('sellprice_parquet.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "462657d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------+-------+--------+--------+---+------+-------------------+--------+--------+----+-----+----+------------+------------+------------+------------+-------+-------+-------+----------+---------+-----------------+\n",
      "|                  id|      item_id|  dept_id| cat_id|store_id|state_id|  d|demand|               date|wm_yr_wk| weekday|wday|month|year|event_name_1|event_type_1|event_name_2|event_type_2|snap_CA|snap_TX|snap_WI|sell_price|     cost|__index_level_0__|\n",
      "+--------------------+-------------+---------+-------+--------+--------+---+------+-------------------+--------+--------+----+-----+----+------------+------------+------------+------------+-------+-------+-------+----------+---------+-----------------+\n",
      "|HOBBIES_1_008_CA_...|HOBBIES_1_008|HOBBIES_1|HOBBIES|    CA_1|      CA|  1|    12|2011-01-29 01:00:00|   11101|Saturday|   1|    1|2011|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false|0.45996094|5.5195312|                7|\n",
      "|HOBBIES_1_009_CA_...|HOBBIES_1_009|HOBBIES_1|HOBBIES|    CA_1|      CA|  1|     2|2011-01-29 01:00:00|   11101|Saturday|   1|    1|2011|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 1.5595703|3.1191406|                8|\n",
      "|HOBBIES_1_010_CA_...|HOBBIES_1_010|HOBBIES_1|HOBBIES|    CA_1|      CA|  1|     0|2011-01-29 01:00:00|   11101|Saturday|   1|    1|2011|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 3.1699219|      0.0|                9|\n",
      "|HOBBIES_1_012_CA_...|HOBBIES_1_012|HOBBIES_1|HOBBIES|    CA_1|      CA|  1|     0|2011-01-29 01:00:00|   11101|Saturday|   1|    1|2011|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 5.9804688|      0.0|               11|\n",
      "|HOBBIES_1_015_CA_...|HOBBIES_1_015|HOBBIES_1|HOBBIES|    CA_1|      CA|  1|     4|2011-01-29 01:00:00|   11101|Saturday|   1|    1|2011|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 0.7001953|2.8007812|               14|\n",
      "+--------------------+-------------+---------+-------+--------+--------+---+------+-------------------+--------+--------+----+-----+----+------------+------------+------------+------------+-------+-------+-------+----------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0adefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------+------+--------+--------+----+------+-------------------+--------+-------+----+-----+----+------------+------------+------------+------------+-------+-------+-------+----------+---------+-----------------+\n",
      "|                  id|    item_id|dept_id|cat_id|store_id|state_id|   d|demand|               date|wm_yr_wk|weekday|wday|month|year|event_name_1|event_type_1|event_name_2|event_type_2|snap_CA|snap_TX|snap_WI|sell_price|     cost|__index_level_0__|\n",
      "+--------------------+-----------+-------+------+--------+--------+----+------+-------------------+--------+-------+----+-----+----+------------+------------+------------+------------+-------+-------+-------+----------+---------+-----------------+\n",
      "|FOODS_3_827_WI_3_...|FOODS_3_827|FOODS_3| FOODS|    WI_3|      WI|1941|     1|2016-05-22 01:00:00|   11617| Sunday|   2|    5|2016|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false|       1.0|      1.0|         59181089|\n",
      "|FOODS_3_826_WI_3_...|FOODS_3_826|FOODS_3| FOODS|    WI_3|      WI|1941|     0|2016-05-22 01:00:00|   11617| Sunday|   2|    5|2016|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 1.2802734|      0.0|         59181088|\n",
      "|FOODS_3_825_WI_3_...|FOODS_3_825|FOODS_3| FOODS|    WI_3|      WI|1941|     2|2016-05-22 01:00:00|   11617| Sunday|   2|    5|2016|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 3.9804688|7.9609375|         59181087|\n",
      "|FOODS_3_824_WI_3_...|FOODS_3_824|FOODS_3| FOODS|    WI_3|      WI|1941|     0|2016-05-22 01:00:00|   11617| Sunday|   2|    5|2016|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 2.4804688|      0.0|         59181086|\n",
      "|FOODS_3_823_WI_3_...|FOODS_3_823|FOODS_3| FOODS|    WI_3|      WI|1941|     1|2016-05-22 01:00:00|   11617| Sunday|   2|    5|2016|     NoEvent|     NoEvent|     NoEvent|     NoEvent|  false|  false|  false| 2.9804688|2.9804688|         59181085|\n",
      "+--------------------+-----------+-------+------+--------+--------+----+------+-------------------+--------+-------+----+-----+----+------------+------------+------------+------------+-------+-------+-------+----------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "model = model.withColumn('index', monotonically_increasing_id())\n",
    "model.orderBy(desc('index')).drop('index').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafedc58",
   "metadata": {},
   "source": [
    "DATA QUALITY TEST - CHECKING FOR COMPLETENESS (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1001cf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+-------------------------------------------------------+-----------------+------------------+\n",
      "|check           |check_level|check_status|constraint                                             |constraint_status|constraint_message|\n",
      "+----------------+-----------+------------+-------------------------------------------------------+-----------------+------------------+\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(id,None))          |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(item_id,None))     |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(dept_id,None))     |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(cat_id,None))      |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(store_id,None))    |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(state_id,None))    |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(d,None))           |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(demand,None))      |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(date,None))        |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(wm_yr_wk,None))    |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(weekday,None))     |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(wday,None))        |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(month,None))       |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(year,None))        |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(event_name_1,None))|Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(event_type_1,None))|Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(event_name_2,None))|Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(event_type_2,None))|Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(snap_CA,None))     |Success          |                  |\n",
      "|Null value Check|Warning    |Success     |CompletenessConstraint(Completeness(snap_TX,None))     |Success          |                  |\n",
      "+----------------+-----------+------------+-------------------------------------------------------+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.verification import *\n",
    "check = Check(spark, CheckLevel.Warning, \"Null value Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(model) \\\n",
    "    .addCheck(  \n",
    "     check.isComplete(\"id\")\\\n",
    "    .isComplete(\"item_id\")\\\n",
    "    .isComplete(\"dept_id\")\\\n",
    "    .isComplete(\"cat_id\")\\\n",
    "    .isComplete(\"store_id\")\\\n",
    "    .isComplete(\"state_id\")\\\n",
    "    .isComplete(\"d\")\\\n",
    "    .isComplete(\"demand\")\\\n",
    "    .isComplete(\"date\")\\\n",
    "    .isComplete(\"wm_yr_wk\")\\\n",
    "    .isComplete(\"weekday\")\\\n",
    "    .isComplete(\"wday\")\\\n",
    "    .isComplete(\"month\")\\\n",
    "    .isComplete(\"year\")\\\n",
    "    .isComplete(\"event_name_1\")\\\n",
    "    .isComplete(\"event_type_1\")\\\n",
    "    .isComplete(\"event_name_2\")\\\n",
    "    .isComplete(\"event_type_2\")\\\n",
    "    .isComplete(\"snap_CA\")\\\n",
    "    .isComplete(\"snap_TX\")\\\n",
    "    .isComplete(\"snap_WI\")\\\n",
    "    .isComplete(\"sell_price\")\\\n",
    "    .isComplete(\"cost\"))\\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962a945",
   "metadata": {},
   "source": [
    "CHECKING IF RANGE OF VALUES ARE VALID i.e. minimum and maximum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372692ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of 'wday':\n",
      "\t Minimum value for wday column is 1.0\n",
      "\t Maximum value for wday column is 7.0\n",
      "Statistics of 'year':\n",
      "\t Minimum value for year column is 2011.0\n",
      "\t Maximum value for year column is 2016.0\n",
      "Statistics of 'cost':\n",
      "\t Minimum value for cost column is 0.0\n",
      "\t Maximum value for cost column is 2164.21875\n",
      "Statistics of 'wm_yr_wk':\n",
      "\t Minimum value for wm_yr_wk column is 11101.0\n",
      "\t Maximum value for wm_yr_wk column is 11617.0\n",
      "Statistics of 'sell_price':\n",
      "\t Minimum value for sell_price column is 0.01000213623046875\n",
      "\t Maximum value for sell_price column is 107.3125\n",
      "Statistics of 'demand':\n",
      "\t Minimum value for demand column is 0.0\n",
      "\t Maximum value for demand column is 763.0\n",
      "Statistics of 'month':\n",
      "\t Minimum value for month column is 1.0\n",
      "\t Maximum value for month column is 12.0\n",
      "Statistics of 'd':\n",
      "\t Minimum value for d column is 1.0\n",
      "\t Maximum value for d column is 1941.0\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.profiles import *\n",
    "\n",
    "result = ColumnProfilerRunner(spark) \\\n",
    "    .onData(model.select('d','demand','wm_yr_wk','wday','month','year','sell_price','cost')) \\\n",
    "    .run()\n",
    "for col, profile in result.profiles.items():\n",
    "    print(f'Statistics of \\'{col}\\':')\n",
    "    print('\\t', f\"Minimum value for {col} column is \"+ str(profile.minimum))\n",
    "    print('\\t', f\"Maximum value for {col} column is \"+ str(profile.maximum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee38b3a",
   "metadata": {},
   "source": [
    "CHECK FOR UNIQUENESS i.e. duplication (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5fdd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n",
      "+----------------------+-----------+------------+------------------------------------------------------+-----------------+------------------+\n",
      "|check                 |check_level|check_status|constraint                                            |constraint_status|constraint_message|\n",
      "+----------------------+-----------+------------+------------------------------------------------------+-----------------+------------------+\n",
      "|Duplication test Check|Warning    |Success     |UniquenessConstraint(Uniqueness(Stream(date, ?),None))|Success          |                  |\n",
      "+----------------------+-----------+------------+------------------------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check if date 'd' are unique. \n",
    "from pydeequ.verification import *\n",
    "check = Check(spark, CheckLevel.Warning, \"Duplication test Check\")\n",
    "checkResult = VerificationSuite(spark).onData(model).addCheck(check\\\n",
    "    .hasUniqueness(['date', 'id'], lambda x:x ==1)).run()\n",
    "                                                           \n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afcdce2",
   "metadata": {},
   "source": [
    "CHECKING FOR ZERO VALUES (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3aefecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+-----------------------------------------------------------------------+-----------------+------------------+\n",
      "|check           |check_level|check_status|constraint                                                             |constraint_status|constraint_message|\n",
      "+----------------+-----------+------------+-----------------------------------------------------------------------+-----------------+------------------+\n",
      "|Zero value Check|Warning    |Success     |ComplianceConstraint(Compliance(Zero value check,d == 0,None))         |Success          |                  |\n",
      "|Zero value Check|Warning    |Success     |ComplianceConstraint(Compliance(Zero value check,wm_yr_wk == 0,None))  |Success          |                  |\n",
      "|Zero value Check|Warning    |Success     |ComplianceConstraint(Compliance(Zero value check,wday == 0,None))      |Success          |                  |\n",
      "|Zero value Check|Warning    |Success     |ComplianceConstraint(Compliance(Zero value check,month == 0,None))     |Success          |                  |\n",
      "|Zero value Check|Warning    |Success     |ComplianceConstraint(Compliance(Zero value check,year == 0,None))      |Success          |                  |\n",
      "|Zero value Check|Warning    |Success     |ComplianceConstraint(Compliance(Zero value check,sell_price == 0,None))|Success          |                  |\n",
      "+----------------+-----------+------------+-----------------------------------------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check = Check(spark, CheckLevel.Warning, \"Zero value Check\")\n",
    "result = VerificationSuite(spark)\\\n",
    ".onData(model)\\\n",
    ".addCheck(check\n",
    "    .satisfies(\"d == 0\", \"Zero value check\", lambda x: x==0)\\\n",
    "    .satisfies(\"wm_yr_wk == 0\", \"Zero value check\", lambda x: x==0)\\\n",
    "    .satisfies(\"wday == 0\", \"Zero value check\", lambda x: x==0)\\\n",
    "    .satisfies(\"month == 0\", \"Zero value check\", lambda x: x==0)\\\n",
    "    .satisfies(\"year == 0\", \"Zero value check\", lambda x: x==0)\\\n",
    "    .satisfies(\"sell_price == 0\", \"Zero value check\", lambda x: x==0)\\\n",
    "    )\\\n",
    ".run()\n",
    "\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6516a",
   "metadata": {},
   "source": [
    "CHECKING FOR NEGATIVE VALUES (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "472f25c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+------------+------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|check          |check_level|check_status|constraint                                                                                                              |constraint_status|constraint_message|\n",
      "+---------------+-----------+------------+------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(d is non-negative,COALESCE(CAST(d AS DECIMAL(20,10)), 0.0) >= 0,None))                  |Success          |                  |\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(demand is non-negative,COALESCE(CAST(demand AS DECIMAL(20,10)), 0.0) >= 0,None))        |Success          |                  |\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(wm_yr_wk is non-negative,COALESCE(CAST(wm_yr_wk AS DECIMAL(20,10)), 0.0) >= 0,None))    |Success          |                  |\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(wday is non-negative,COALESCE(CAST(wday AS DECIMAL(20,10)), 0.0) >= 0,None))            |Success          |                  |\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(month is non-negative,COALESCE(CAST(month AS DECIMAL(20,10)), 0.0) >= 0,None))          |Success          |                  |\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(year is non-negative,COALESCE(CAST(year AS DECIMAL(20,10)), 0.0) >= 0,None))            |Success          |                  |\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(sell_price is non-negative,COALESCE(CAST(sell_price AS DECIMAL(20,10)), 0.0) >= 0,None))|Success          |                  |\n",
      "|Negative Values|Warning    |Success     |ComplianceConstraint(Compliance(cost is non-negative,COALESCE(CAST(cost AS DECIMAL(20,10)), 0.0) >= 0,None))            |Success          |                  |\n",
      "+---------------+-----------+------------+------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkResult = VerificationSuite(spark) \\\n",
    "                    .onData(model) \\\n",
    "                    .addCheck(\n",
    "                        Check(spark, CheckLevel.Warning, \"Negative Values\")\\\n",
    "                            .isNonNegative('d')\\\n",
    "                            .isNonNegative('demand')\\\n",
    "                            .isNonNegative('wm_yr_wk')\\\n",
    "                            .isNonNegative('wday')\\\n",
    "                            .isNonNegative('month')\\\n",
    "                            .isNonNegative('year')\\\n",
    "                            .isNonNegative('sell_price')\\\n",
    "                            .isNonNegative('cost')\\\n",
    "                            )\\\n",
    "                            .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e6e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c2d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8f0bf4693b58d1f9a5b430ad72ab89eac35cd395614662f911e64fe02b4d10e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
