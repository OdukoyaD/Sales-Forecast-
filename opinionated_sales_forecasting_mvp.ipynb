{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6bf71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libriries to use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn import metrics\n",
    "#from sklearn.tree import export_graphviz\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#from IPython.display import Image  \n",
    "#import pydotplus\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658cc041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f24f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.3.0-bin-hadoop3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e116d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\__init__.py\n",
      "C:\\Spark\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "print(pyspark.__file__)\n",
    "path = os.path.abspath(pyspark.__file__)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f13b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pydeequ\n",
    "#from pydeequ.analyzers import *\n",
    "#from pydeequ.profiles import *\n",
    "#from pydeequ.suggestions import *\n",
    "#from pydeequ.checks import *\n",
    "#from pydeequ.verification import *\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "#from pyspark.sql import functions as F\n",
    "#from pyspark.sql.types import DecimalType, DoubleType, IntegerType, DateType, NumericType, StructType, StringType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5df94c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (293502260.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [8]\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\"\u001b[0m\n\u001b[1;37m        \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79e1adb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-GUQ0P1V:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1946b900f40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96540cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING CALENDER CSV\n",
    "df = pd.read_csv(\"calendar.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a529f2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  wm_yr_wk    weekday  wday  month  year    d event_name_1  \\\n",
       "0  2011-01-29     11101   Saturday     1      1  2011  d_1          NaN   \n",
       "1  2011-01-30     11101     Sunday     2      1  2011  d_2          NaN   \n",
       "2  2011-01-31     11101     Monday     3      1  2011  d_3          NaN   \n",
       "3  2011-02-01     11101    Tuesday     4      2  2011  d_4          NaN   \n",
       "4  2011-02-02     11101  Wednesday     5      2  2011  d_5          NaN   \n",
       "\n",
       "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n",
       "0          NaN          NaN          NaN        0        0        0  \n",
       "1          NaN          NaN          NaN        0        0        0  \n",
       "2          NaN          NaN          NaN        0        0        0  \n",
       "3          NaN          NaN          NaN        1        1        0  \n",
       "4          NaN          NaN          NaN        1        0        1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8e5702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "|      date|wm_yr_wk|  weekday|wday|month|year|  d|event_name_1|event_type_1|event_name_2|event_type_2|snap_CA|snap_TX|snap_WI|\n",
      "+----------+--------+---------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "|2011-01-29|   11101| Saturday|   1|    1|2011|d_1|        null|        null|        null|        null|      0|      0|      0|\n",
      "|2011-01-30|   11101|   Sunday|   2|    1|2011|d_2|        null|        null|        null|        null|      0|      0|      0|\n",
      "|2011-01-31|   11101|   Monday|   3|    1|2011|d_3|        null|        null|        null|        null|      0|      0|      0|\n",
      "|2011-02-01|   11101|  Tuesday|   4|    2|2011|d_4|        null|        null|        null|        null|      1|      1|      0|\n",
      "|2011-02-02|   11101|Wednesday|   5|    2|2011|d_5|        null|        null|        null|        null|      1|      0|      1|\n",
      "+----------+--------+---------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spk = spark.read.csv('calendar.csv', header=True)\n",
    "df_spk.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f613c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- wm_yr_wk: string (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- wday: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- d: string (nullable = true)\n",
      " |-- event_name_1: string (nullable = true)\n",
      " |-- event_type_1: string (nullable = true)\n",
      " |-- event_name_2: string (nullable = true)\n",
      " |-- event_type_2: string (nullable = true)\n",
      " |-- snap_CA: string (nullable = true)\n",
      " |-- snap_TX: string (nullable = true)\n",
      " |-- snap_WI: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spk.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4dddee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- wm_yr_wk: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- wday: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- d: string (nullable = true)\n",
      " |-- event_name_1: string (nullable = true)\n",
      " |-- event_type_1: string (nullable = true)\n",
      " |-- event_name_2: string (nullable = true)\n",
      " |-- event_type_2: string (nullable = true)\n",
      " |-- snap_CA: boolean (nullable = true)\n",
      " |-- snap_TX: boolean (nullable = true)\n",
      " |-- snap_WI: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType\n",
    "df_spk = df_spk.withColumn(\"date\",col(\"date\").cast(DateType())) \\\n",
    "    .withColumn(\"wm_yr_wk\",col(\"wm_yr_wk\").cast(IntegerType())) \\\n",
    "    .withColumn(\"weekday\",col(\"weekday\").cast(StringType())) \\\n",
    "    .withColumn(\"wday\",col(\"wday\").cast(IntegerType())) \\\n",
    "    .withColumn(\"month\",col(\"month\").cast(IntegerType())) \\\n",
    "    .withColumn(\"year\",col(\"year\").cast(IntegerType())) \\\n",
    "    .withColumn(\"d\",col(\"d\").cast(StringType())) \\\n",
    "    .withColumn(\"event_name_1\",col(\"event_name_1\").cast(StringType())) \\\n",
    "    .withColumn(\"event_type_1\",col(\"event_type_1\").cast(StringType())) \\\n",
    "    .withColumn(\"event_name_2\",col(\"event_name_2\").cast(StringType())) \\\n",
    "    .withColumn(\"event_type_2\",col(\"event_type_2\").cast(StringType())) \\\n",
    "    .withColumn(\"snap_CA\",col(\"snap_CA\").cast(BooleanType())) \\\n",
    "    .withColumn(\"snap_TX\",col(\"snap_TX\").cast(BooleanType())) \\\n",
    "    .withColumn(\"snap_WI\",col(\"snap_WI\").cast(BooleanType()))\n",
    "df_spk.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f9ffbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "|      date|wm_yr_wk|  weekday|wday|month|year|  d|event_name_1|event_type_1|event_name_2|event_type_2|snap_CA|snap_TX|snap_WI|\n",
      "+----------+--------+---------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "|2011-01-29|   11101| Saturday|   1|    1|2011|d_1|        null|        null|        null|        null|  false|  false|  false|\n",
      "|2011-01-30|   11101|   Sunday|   2|    1|2011|d_2|        null|        null|        null|        null|  false|  false|  false|\n",
      "|2011-01-31|   11101|   Monday|   3|    1|2011|d_3|        null|        null|        null|        null|  false|  false|  false|\n",
      "|2011-02-01|   11101|  Tuesday|   4|    2|2011|d_4|        null|        null|        null|        null|   true|   true|  false|\n",
      "|2011-02-02|   11101|Wednesday|   5|    2|2011|d_5|        null|        null|        null|        null|   true|  false|   true|\n",
      "+----------+--------+---------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spk.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5881dc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'wm_yr_wk',\n",
       " 'weekday',\n",
       " 'wday',\n",
       " 'month',\n",
       " 'year',\n",
       " 'd',\n",
       " 'event_name_1',\n",
       " 'event_type_1',\n",
       " 'event_name_2',\n",
       " 'event_type_2',\n",
       " 'snap_CA',\n",
       " 'snap_TX',\n",
       " 'snap_WI']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ae86265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "|date|wm_yr_wk|weekday|wday|month|year|  d|event_name_1|event_type_1|event_name_2|event_type_2|snap_CA|snap_TX|snap_WI|\n",
      "+----+--------+-------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "|   0|       0|      0|   0|    0|   0|  0|        1807|        1807|        1964|        1964|      0|      0|      0|\n",
      "+----+--------+-------+----+-----+----+---+------------+------------+------------+------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spk_Val = df_spk.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_spk.columns])\n",
    "df_spk_Val.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c56a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+----+-----+----+----+------------+------------+------------+------------+-------+-------+-------+\n",
      "|date|wm_yr_wk|weekday|wday|month|year|   d|event_name_1|event_type_1|event_name_2|event_type_2|snap_CA|snap_TX|snap_WI|\n",
      "+----+--------+-------+----+-----+----+----+------------+------------+------------+------------+-------+-------+-------+\n",
      "|1969|    1969|   1969|1969| 1969|1969|1969|         162|         162|           5|           5|   1969|   1969|   1969|\n",
      "+----+--------+-------+----+-----+----+----+------------+------------+------------+------------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spk_Null_Val = df_spk.select([F.count(F.when(F.col(c).isNotNull(), c)).alias(c) for c in df_spk.columns])\n",
    "df_spk_Null_Val.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a3a04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5ed47ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe is 9.07028 MB\n",
      "partitions is 2\n"
     ]
    }
   ],
   "source": [
    "rdd = df_spk_Val.rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "obj = rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
    "size = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(obj)\n",
    "size_MB = size/1000000\n",
    "partitions = max(int(size_MB/200), 2)\n",
    "print(f'The dataframe is {size_MB} MB')\n",
    "print(f'partitions is {partitions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4baa2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your code here\n",
    "df_spk_Val.coalesce(partitions).write.parquet('calendar_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5177aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
